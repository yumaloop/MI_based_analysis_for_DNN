{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Paper below:\n",
    "    - \"Approximating Mutual Information by Maximum Likelihood Density Ratio Estimation\", T. Suzuki et al. 2008\n",
    "       http://proceedings.mlr.press/v4/suzuki08a/suzuki08a.pdf\n",
    "       \n",
    "  https://qiita.com/wsuzume/items/09a59036c8944fd563ff#%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E5%9B%9E%E5%B8%B0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カーネル法を用いた相互情報量推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カーネル法を用いて密度比$r(x, y)$の推定を行い，サンプルから変数$X, Y$の相互情報量の推定値$\\hat{MI}(X, Y)$を求める．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相互情報量\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    MI(X, Y) &:= \\int_{X} \\int_{Y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)} ~ dydx \\\\\n",
    "    &= \\mathbb{E}_{XY} \\left[ \\log \\frac{p(x, y)}{p(x)p(y)} \\right] \\\\\n",
    "    &\\simeq\t \\frac{1}{n} \\sum_{i=1}^{n} \\log \\frac{p(x_i, y_i)}{p(x_i)p(y_i)} ~~~ (n \\to \\infty) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ここで，密度比を\n",
    "\n",
    "$$\n",
    "    r(x, y) := \\frac{p(x, y)}{p(x)p(y)}\n",
    "$$\n",
    "\n",
    "とおけば，\n",
    "\n",
    "$$\n",
    "    MI(X, Y) \\simeq\t \\frac{1}{n} \\sum_{i=1}^{n} \\log r(x, y) ~~~ (n \\to \\infty) \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 密度比 $r(x, y)$ の推定\n",
    "\n",
    "#### 線形基底回帰\n",
    "\n",
    "基底関数 $\\phi: X^n \\times Y^n \\to \\mathbb{R}^{d}$ と線形重み$\\boldsymbol{\\beta} \\in \\mathbb{R}^{d}$を用いて，密度比$r(x, y)$を近似する．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r(x, y) &\\simeq \\hat{r}(x, y) \\\\\n",
    "    &= \\boldsymbol{\\beta}^{\\mathrm T} \\phi(x, y)  \\\\\n",
    "    &= \\sum_{i=1}^{d} \\beta_i \\cdot \\phi_i(x, y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### カーネル回帰\n",
    "\n",
    "これはカーネル法を使うと以下のようになる．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r(x, y) &\\simeq \\hat{r}(x, y) \\\\\n",
    "    &= \\sum_{i=1}^{n} \\alpha_i \\cdot k \\left( {({x}^{(i)}, {y}^{(i)})}^{\\mathrm T}, {(x, y)}^{\\mathrm T} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "たとえば，$k$として，RBFカーネルを用いれば\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    k_{RBF}(x, x') \n",
    "    &= \\exp \\left( - \\frac{{|| x - x' || }^2}{2 \\sigma^2} \\right) = \\exp \\left( - \\gamma {|| x - x' || }^2 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "代入して，\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    r(x, y) &\\simeq \\hat{r}(x, y) \\\\\n",
    "    &= \\sum_{i=1}^{n} \\alpha_i \\cdot k \\left( {({x}^{(i)}, {y}^{(i)})}^{\\mathrm T}, {(x, y)}^{\\mathrm T} \\right) \\\\\n",
    "    &= \\sum_{i=1}^{n} \\alpha_i \\cdot \\exp \\left( - \\gamma { || \\left( \\begin{array}{c} x^{(i)} \\\\ y^{(i)} \\end{array} \\right) - \\left( \\begin{array}{c} x \\\\ y \\end{array} \\right) || }^2 \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "ハイパーパラメータ$\\alpha$の探索のために勾配法を用いる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInally, we get the estimator $\\hat{MI}(X, Y)$ as follows. Note that $\\alpha$ and $\\gamma$ is the hyperparameters.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{MI}(X, Y) &= \\frac{1}{n} \\sum_{i=1}^{n} \\log \\hat{r}(x_i y_i) \\\\ \n",
    "    &= \\frac{1}{n} \\sum_{i=1}^{n} \\log \\sum_{i=1}^{n} \\alpha_i \\cdot \\exp \\left( - \\gamma { || \\left( \\begin{array}{c} x^{(i)} \\\\ y^{(i)} \\end{array} \\right) - \\left( \\begin{array}{c} x \\\\ y \\end{array} \\right) || }^2 \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNNの第$i$層:$L_i$および第$j$層:$L_j$の間の相互情報量は，$N$コのサンプル$\\{ (x^{(n)}, y^{(n)}) \\}_{n=1 \\sim N}$から以下の式で推定できる\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\hat{MI}(L_i, L_j) \n",
    "    &= \\frac{1}{N} \\sum_{n=1}^{N} \\log \\hat{r}({l_i}^{(n)}, {l_j}^{(n)}) \\\\ \n",
    "    &= \\frac{1}{N} \\sum_{n=1}^{N} \\log \\sum_{n=1}^{N} \\alpha_n \\cdot \\exp \\left( - \\gamma { || \\left( \\begin{array}{c} {l_i}^{(n)} \\\\ {l_j}^{(n)} \\end{array} \\right) - \\left( \\begin{array}{c} l_i \\\\ l_j \\end{array} \\right) || }^2 \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for optimize alpha\n",
    "\n",
    "ある変数$X, Y$に対して，$n$コのサンプル$\\{ (x^{(i)}, y^{(i)}) \\}_{i=1 \\sim n}$から，相互情報量$MI(X, Y)$の推定量を求める．\n",
    "\n",
    "1. initialize $\\boldsymbol{\\alpha} = {(\\alpha_1, \\dots, \\alpha_n)}^{\\mathrm T}$. i.e. assign mean value for all. $\\alpha_i = \\frac{1}{n}$\n",
    "\n",
    "2. foreach step $t$ <br>\n",
    "    2.1. calculate the log-likelihood $\\hat{p}$ of sample $\\{ (x^{(i)}, y^{(i)}) \\}_{i=1 \\sim n}$\n",
    "    $$\n",
    "            \\alpha_{opt} = \\underset{\\alpha}{\\rm argmax} \\sum_{i=1}^{n} \\log \\sum_{j=1}^{n} \\alpha_j \\cdot k \\left( {({x}^{(i)}, {y}^{(i)})}^{\\mathrm T}, {(x^{(j)}, y^{(j)})}^{\\mathrm T} \\right)\n",
    "            $$\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBFカーネル\n",
    "class GaussianKernel(object):\n",
    "    \n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def get_sigma(self):\n",
    "        return np.copy(self.sigma)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.exp( (-1. / (self.sigma ** 2) ) *  (x - self.mu) ** 2)\n",
    "\n",
    "    # シグマによる，RBFカーネルの偏導関数\n",
    "    def derivatives(self, x1, x2):\n",
    "        dif_sigma = np.exp( (-1. / (self.sigma ** 2) ) *  (x1 - x2) ** 2) * ( (x1 - x2) ** 2 ) / ( self.sigma ** 3)\n",
    "        return dif_sigma\n",
    "\n",
    "    # シグマに差分を足しこむ．\n",
    "    def update_sigma(self, update):\n",
    "        self.sigma += update\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityRatioEstimater():\n",
    "    \n",
    "    def __init__(self, x_sample, y_sample, b=200, K=50)\n",
    "        \"\"\"\n",
    "        x_sample: data samples of the variable X\n",
    "        y_sample: data samples of the variable Y\n",
    "        z_sample: combine of x_sample & y_sample\n",
    "        kernel: kernel function (Gaussian kernel is typical)\n",
    "        b: number of basis function\n",
    "        K: number of subset of samples. used for CrossValidation.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x_sample = x_sample\n",
    "        self.y_sample = y_sample\n",
    "        self.z_sample = np.append(x_sample, y_sample)\n",
    "        self.b = b\n",
    "        self.K = K\n",
    "        self.sample_size = len(x_sample)\n",
    "        \n",
    "        \n",
    "    def CrossValidation(self):\n",
    "        basis_functions = self._make_basisfunctions()\n",
    "        \n",
    "        \n",
    "    def _make_basisfunctions(self):\n",
    "        # b個のbasis function (GaussianKernel)を作成する．\n",
    "        # choose the data sample randomly from z_sample\n",
    "        \n",
    "        # seed set\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        basis_functions = []\n",
    "        for i in range(self.b):\n",
    "            rand_id = np.random.randint(0, self.sample_size)\n",
    "            basis = GaussianKernel(mu=self.z_sample[rand_id], sigma=2.)\n",
    "            bassi_functions.append(kernel)\n",
    "        return basis_functions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.04229391])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 * np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessRegression(object):\n",
    "\n",
    "    def __init__(self, kernel, beta=1.):\n",
    "        self.kernel = kernel\n",
    "        self.beta = beta\n",
    "\n",
    "    def fit(self, x, t):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        Gram = self.kernel(*np.meshgrid(x, x))\n",
    "        self.covariance = Gram + np.identity(len(x)) / self.beta\n",
    "        self.precision = np.linalg.inv(self.covariance)\n",
    "\n",
    "    def fit_kernel(self, x, t, learning_rate=0.1, iter_max=10000):\n",
    "        for i in range(iter_max):\n",
    "            params = self.kernel.get_params()\n",
    "            self.fit(x, t)\n",
    "            gradients = self.kernel.derivatives(*np.meshgrid(x, x))\n",
    "            updates = np.array(\n",
    "                [-np.trace(self.precision.dot(grad)) + t.dot(self.precision.dot(grad).dot(self.precision).dot(t)) for grad in gradients])\n",
    "            self.kernel.update_parameters(learning_rate * updates)\n",
    "            if np.allclose(params, self.kernel.get_params()):\n",
    "                break\n",
    "        else:\n",
    "            print(\"parameters may not have converged\")\n",
    "\n",
    "    def predict_dist(self, x):\n",
    "        K = self.kernel(*np.meshgrid(x, self.x, indexing='ij'))\n",
    "        mean = K.dot(self.precision).dot(self.t)\n",
    "        var = self.kernel(x, x) + 1 / self.beta - np.sum(K.dot(self.precision) * K, axis=1)\n",
    "        return mean.ravel(), np.sqrt(var.ravel())\n",
    "\n",
    "\n",
    "def create_toy_data(func, low=0, high=1., n=10, std=1.):\n",
    "    x = np.random.uniform(low, high, n)\n",
    "    t = func(x) + np.random.normal(scale=std, size=n)\n",
    "    return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    def func(x):\n",
    "        return np.sin(2 * np.pi * x)\n",
    "\n",
    "    x, t = create_toy_data(func, high=0.7, std=0.1)\n",
    "\n",
    "    kernel = GaussianKernel(params=np.array([1., 1.]))\n",
    "    regression = GaussianProcessRegression(kernel=kernel, beta=100)\n",
    "    regression.fit_kernel(x, t, learning_rate=0.1, iter_max=10000)\n",
    "\n",
    "    x_test = np.linspace(0, 1, 100)\n",
    "    y, y_std = regression.predict_dist(x_test)\n",
    "\n",
    "    plt.scatter(x, t, alpha=0.5, color=\"blue\", label=\"observation\")\n",
    "    plt.plot(x_test, func(x_test), color=\"blue\", label=\"sin$(2\\pi x)$\")\n",
    "    plt.plot(x_test, y, color=\"red\", label=\"predict_mean\")\n",
    "    plt.fill_between(x_test, y - y_std, y + y_std, color=\"pink\", alpha=0.5, label=\"predict_std\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
