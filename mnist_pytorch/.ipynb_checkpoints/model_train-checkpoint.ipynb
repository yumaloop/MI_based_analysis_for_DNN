{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic CNN Model for MNIST dataset\n",
    "\n",
    "---\n",
    "\n",
    "MINEによって，layer間の相互情報量(MI; Mututla Information)を測る．そのために，学習済みモデルを用意しておく．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from models import DNN, CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Info\n",
      "===========\n",
      "- use_cuda:  True\n",
      "- Path:  /home/uchiumi/JNNS2019/mnist_pytorch\n",
      "- PyTorch 1.0.1.post2\n",
      "- Python:  3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Setting Info\")\n",
    "print(\"===========\")\n",
    "print(\"- use_cuda: \", use_cuda)\n",
    "print(\"- Path: \", os.getcwd())\n",
    "print(\"- PyTorch\", torch.__version__)\n",
    "print(\"- Python: \", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "log_interval = 100\n",
    "\n",
    "# \n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data', \n",
    "                             train=True, \n",
    "                             download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./data', \n",
    "                             train=False, \n",
    "                             download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "date_string = datetime.now().strftime(\"%Y-%m%d-%H%M\")\n",
    "model_name = \"dnn_mnist__\" + date_string\n",
    "optim_name = \"optim__\" + date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for log plot\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i * len(train_loader.dataset) for i in range(1, n_epochs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[\"model_output\"]\n",
    "        train_loss = F.nll_loss(output, target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0 and batch_idx != 0:\n",
    "            print('epoch: {:>2} [{}/{} ]\\t train loss: {:.4f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), train_loss.item()))\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            train_counter.append((batch_idx * batch_size_train) + ((epoch - 1)*len(train_loader.dataset)))\n",
    "            \n",
    "            model_path = os.path.join(os.getcwd(), \"train_log\", model_name + \".pth\")\n",
    "            optim_path = os.path.join(os.getcwd(), \"train_log\", optim_name + \".pth\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            torch.save(optimizer.state_dict(), optim_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(model, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    nb_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)[\"model_output\"]\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            test_batch_loss = F.nll_loss(output, target, size_average=False).item() / batch_size_test\n",
    "            # test_loss /= len(test_loader.dataset)\n",
    "            # test_losses.append(test_loss)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            nb_correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            accuracy = (float(nb_correct) / float(batch_size_test * (batch_idx + 1)) ) * 100.0\n",
    "            \n",
    "        print('epoch: {:>2} avg. test loss: {:.4f}, test acc: {:.2f}% ({}/{})'.format(epoch, test_batch_loss, accuracy, nb_correct, batch_size_test * (batch_idx + 1)))\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "model = DNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uchiumi/JNNS2019/mnist_pytorch/models.py:61: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.fc5(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [12800/60000 ]\t train loss: 2.2775\n",
      "epoch: 1 [25600/60000 ]\t train loss: 2.1252\n",
      "epoch: 1 [38400/60000 ]\t train loss: 1.1783\n",
      "epoch: 1 [51200/60000 ]\t train loss: 0.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uchiumi/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 avg. test loss: 0.5179, test acc: 85.58% (8558/10000)\n",
      "epoch: 2 [12800/60000 ]\t train loss: 0.4555\n",
      "epoch: 2 [25600/60000 ]\t train loss: 0.4652\n",
      "epoch: 2 [38400/60000 ]\t train loss: 0.3576\n",
      "epoch: 2 [51200/60000 ]\t train loss: 0.2320\n",
      "epoch: 2 avg. test loss: 0.2997, test acc: 91.46% (9146/10000)\n",
      "epoch: 3 [12800/60000 ]\t train loss: 0.2487\n",
      "epoch: 3 [25600/60000 ]\t train loss: 0.2275\n",
      "epoch: 3 [38400/60000 ]\t train loss: 0.3058\n",
      "epoch: 3 [51200/60000 ]\t train loss: 0.2035\n",
      "epoch: 3 avg. test loss: 0.2536, test acc: 93.37% (9337/10000)\n",
      "epoch: 4 [12800/60000 ]\t train loss: 0.2810\n",
      "epoch: 4 [25600/60000 ]\t train loss: 0.1517\n",
      "epoch: 4 [38400/60000 ]\t train loss: 0.1315\n",
      "epoch: 4 [51200/60000 ]\t train loss: 0.2138\n",
      "epoch: 4 avg. test loss: 0.2003, test acc: 94.67% (9467/10000)\n",
      "epoch: 5 [12800/60000 ]\t train loss: 0.2155\n",
      "epoch: 5 [25600/60000 ]\t train loss: 0.1265\n",
      "epoch: 5 [38400/60000 ]\t train loss: 0.2138\n",
      "epoch: 5 [51200/60000 ]\t train loss: 0.0743\n",
      "epoch: 5 avg. test loss: 0.1551, test acc: 95.56% (9556/10000)\n",
      "epoch: 6 [12800/60000 ]\t train loss: 0.1544\n",
      "epoch: 6 [25600/60000 ]\t train loss: 0.1714\n",
      "epoch: 6 [38400/60000 ]\t train loss: 0.1071\n",
      "epoch: 6 [51200/60000 ]\t train loss: 0.1859\n",
      "epoch: 6 avg. test loss: 0.1526, test acc: 96.12% (9612/10000)\n",
      "epoch: 7 [12800/60000 ]\t train loss: 0.0618\n",
      "epoch: 7 [25600/60000 ]\t train loss: 0.0907\n",
      "epoch: 7 [38400/60000 ]\t train loss: 0.0900\n",
      "epoch: 7 [51200/60000 ]\t train loss: 0.1165\n",
      "epoch: 7 avg. test loss: 0.0963, test acc: 96.65% (9665/10000)\n",
      "epoch: 8 [12800/60000 ]\t train loss: 0.0500\n",
      "epoch: 8 [25600/60000 ]\t train loss: 0.1104\n",
      "epoch: 8 [38400/60000 ]\t train loss: 0.0693\n",
      "epoch: 8 [51200/60000 ]\t train loss: 0.1280\n",
      "epoch: 8 avg. test loss: 0.1278, test acc: 96.76% (9676/10000)\n",
      "epoch: 9 [12800/60000 ]\t train loss: 0.1032\n",
      "epoch: 9 [25600/60000 ]\t train loss: 0.0884\n",
      "epoch: 9 [38400/60000 ]\t train loss: 0.1291\n",
      "epoch: 9 [51200/60000 ]\t train loss: 0.0799\n",
      "epoch: 9 avg. test loss: 0.0962, test acc: 96.93% (9693/10000)\n",
      "epoch: 10 [12800/60000 ]\t train loss: 0.0735\n",
      "epoch: 10 [25600/60000 ]\t train loss: 0.0871\n",
      "epoch: 10 [38400/60000 ]\t train loss: 0.0485\n",
      "epoch: 10 [51200/60000 ]\t train loss: 0.0445\n",
      "epoch: 10 avg. test loss: 0.1031, test acc: 97.16% (9716/10000)\n",
      "epoch: 11 [12800/60000 ]\t train loss: 0.0476\n",
      "epoch: 11 [25600/60000 ]\t train loss: 0.0589\n",
      "epoch: 11 [38400/60000 ]\t train loss: 0.0464\n",
      "epoch: 11 [51200/60000 ]\t train loss: 0.0217\n",
      "epoch: 11 avg. test loss: 0.0807, test acc: 97.38% (9738/10000)\n",
      "epoch: 12 [12800/60000 ]\t train loss: 0.0202\n",
      "epoch: 12 [25600/60000 ]\t train loss: 0.0322\n",
      "epoch: 12 [38400/60000 ]\t train loss: 0.0368\n",
      "epoch: 12 [51200/60000 ]\t train loss: 0.0589\n",
      "epoch: 12 avg. test loss: 0.0865, test acc: 97.32% (9732/10000)\n",
      "epoch: 13 [12800/60000 ]\t train loss: 0.0922\n",
      "epoch: 13 [25600/60000 ]\t train loss: 0.0272\n",
      "epoch: 13 [38400/60000 ]\t train loss: 0.0377\n",
      "epoch: 13 [51200/60000 ]\t train loss: 0.0387\n",
      "epoch: 13 avg. test loss: 0.0907, test acc: 97.68% (9768/10000)\n",
      "epoch: 14 [12800/60000 ]\t train loss: 0.0185\n",
      "epoch: 14 [25600/60000 ]\t train loss: 0.0388\n",
      "epoch: 14 [38400/60000 ]\t train loss: 0.0341\n",
      "epoch: 14 [51200/60000 ]\t train loss: 0.0113\n",
      "epoch: 14 avg. test loss: 0.0804, test acc: 97.56% (9756/10000)\n",
      "epoch: 15 [12800/60000 ]\t train loss: 0.0277\n",
      "epoch: 15 [25600/60000 ]\t train loss: 0.0933\n",
      "epoch: 15 [38400/60000 ]\t train loss: 0.0455\n",
      "epoch: 15 [51200/60000 ]\t train loss: 0.0166\n",
      "epoch: 15 avg. test loss: 0.0766, test acc: 97.76% (9776/10000)\n",
      "epoch: 16 [12800/60000 ]\t train loss: 0.0166\n",
      "epoch: 16 [25600/60000 ]\t train loss: 0.0389\n",
      "epoch: 16 [38400/60000 ]\t train loss: 0.0445\n",
      "epoch: 16 [51200/60000 ]\t train loss: 0.0215\n",
      "epoch: 16 avg. test loss: 0.0579, test acc: 97.56% (9756/10000)\n",
      "epoch: 17 [12800/60000 ]\t train loss: 0.0221\n",
      "epoch: 17 [25600/60000 ]\t train loss: 0.1339\n",
      "epoch: 17 [38400/60000 ]\t train loss: 0.0178\n",
      "epoch: 17 [51200/60000 ]\t train loss: 0.0173\n",
      "epoch: 17 avg. test loss: 0.0802, test acc: 97.73% (9773/10000)\n",
      "epoch: 18 [12800/60000 ]\t train loss: 0.0075\n",
      "epoch: 18 [25600/60000 ]\t train loss: 0.0107\n",
      "epoch: 18 [38400/60000 ]\t train loss: 0.0067\n",
      "epoch: 18 [51200/60000 ]\t train loss: 0.0289\n",
      "epoch: 18 avg. test loss: 0.0679, test acc: 97.86% (9786/10000)\n",
      "epoch: 19 [12800/60000 ]\t train loss: 0.0490\n",
      "epoch: 19 [25600/60000 ]\t train loss: 0.0161\n",
      "epoch: 19 [38400/60000 ]\t train loss: 0.0114\n",
      "epoch: 19 [51200/60000 ]\t train loss: 0.0104\n",
      "epoch: 19 avg. test loss: 0.0861, test acc: 97.77% (9777/10000)\n",
      "epoch: 20 [12800/60000 ]\t train loss: 0.0062\n",
      "epoch: 20 [25600/60000 ]\t train loss: 0.0052\n",
      "epoch: 20 [38400/60000 ]\t train loss: 0.0057\n",
      "epoch: 20 [51200/60000 ]\t train loss: 0.0105\n",
      "epoch: 20 avg. test loss: 0.0588, test acc: 97.75% (9775/10000)\n",
      "epoch: 21 [12800/60000 ]\t train loss: 0.0068\n",
      "epoch: 21 [25600/60000 ]\t train loss: 0.0063\n",
      "epoch: 21 [38400/60000 ]\t train loss: 0.0027\n",
      "epoch: 21 [51200/60000 ]\t train loss: 0.0206\n",
      "epoch: 21 avg. test loss: 0.0941, test acc: 97.88% (9788/10000)\n",
      "epoch: 22 [12800/60000 ]\t train loss: 0.0028\n",
      "epoch: 22 [25600/60000 ]\t train loss: 0.0140\n",
      "epoch: 22 [38400/60000 ]\t train loss: 0.0073\n",
      "epoch: 22 [51200/60000 ]\t train loss: 0.0099\n",
      "epoch: 22 avg. test loss: 0.0606, test acc: 97.70% (9770/10000)\n",
      "epoch: 23 [12800/60000 ]\t train loss: 0.0082\n",
      "epoch: 23 [25600/60000 ]\t train loss: 0.0125\n",
      "epoch: 23 [38400/60000 ]\t train loss: 0.0084\n",
      "epoch: 23 [51200/60000 ]\t train loss: 0.0038\n",
      "epoch: 23 avg. test loss: 0.0863, test acc: 97.95% (9795/10000)\n",
      "epoch: 24 [12800/60000 ]\t train loss: 0.0093\n",
      "epoch: 24 [25600/60000 ]\t train loss: 0.0046\n",
      "epoch: 24 [38400/60000 ]\t train loss: 0.0103\n",
      "epoch: 24 [51200/60000 ]\t train loss: 0.0098\n",
      "epoch: 24 avg. test loss: 0.0714, test acc: 97.94% (9794/10000)\n",
      "epoch: 25 [12800/60000 ]\t train loss: 0.0022\n",
      "epoch: 25 [25600/60000 ]\t train loss: 0.0054\n",
      "epoch: 25 [38400/60000 ]\t train loss: 0.0048\n",
      "epoch: 25 [51200/60000 ]\t train loss: 0.0042\n",
      "epoch: 25 avg. test loss: 0.1124, test acc: 97.81% (9781/10000)\n",
      "epoch: 26 [12800/60000 ]\t train loss: 0.0072\n",
      "epoch: 26 [25600/60000 ]\t train loss: 0.0011\n",
      "epoch: 26 [38400/60000 ]\t train loss: 0.0103\n",
      "epoch: 26 [51200/60000 ]\t train loss: 0.0022\n",
      "epoch: 26 avg. test loss: 0.0664, test acc: 97.95% (9795/10000)\n",
      "epoch: 27 [12800/60000 ]\t train loss: 0.0050\n",
      "epoch: 27 [25600/60000 ]\t train loss: 0.0032\n",
      "epoch: 27 [38400/60000 ]\t train loss: 0.0023\n",
      "epoch: 27 [51200/60000 ]\t train loss: 0.0016\n",
      "epoch: 27 avg. test loss: 0.0949, test acc: 97.87% (9787/10000)\n",
      "epoch: 28 [12800/60000 ]\t train loss: 0.0026\n",
      "epoch: 28 [25600/60000 ]\t train loss: 0.0019\n",
      "epoch: 28 [38400/60000 ]\t train loss: 0.0270\n",
      "epoch: 28 [51200/60000 ]\t train loss: 0.0031\n",
      "epoch: 28 avg. test loss: 0.0802, test acc: 97.85% (9785/10000)\n",
      "epoch: 29 [12800/60000 ]\t train loss: 0.0035\n",
      "epoch: 29 [25600/60000 ]\t train loss: 0.0024\n",
      "epoch: 29 [38400/60000 ]\t train loss: 0.0029\n",
      "epoch: 29 [51200/60000 ]\t train loss: 0.0040\n",
      "epoch: 29 avg. test loss: 0.0807, test acc: 97.94% (9794/10000)\n",
      "epoch: 30 [12800/60000 ]\t train loss: 0.0017\n",
      "epoch: 30 [25600/60000 ]\t train loss: 0.0033\n",
      "epoch: 30 [38400/60000 ]\t train loss: 0.0045\n",
      "epoch: 30 [51200/60000 ]\t train loss: 0.0015\n",
      "epoch: 30 avg. test loss: 0.0540, test acc: 97.91% (9791/10000)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    model_fit(model, optimizer, epoch)\n",
    "    model_validation(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (30,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5e7c0e1bb3ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of training examples seen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m     return gca().plot(\n\u001b[1;32m   2810\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2811\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (30,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.plot(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn_mnist__2019-0425-1923\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
